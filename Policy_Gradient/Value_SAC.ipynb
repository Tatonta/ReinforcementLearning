{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.distributions import Normal\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tatonta\\AppData\\Local\\Temp\\ipykernel_1444\\1367870582.py:9: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self.dones = np.zeros(self.mem_size, dtype = np.bool)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0  score -57.59  100 game average -57.59\n",
      "episode  1  score -485.57  100 game average -271.58\n",
      "episode  2  score -43.06  100 game average -195.40\n",
      "episode  3  score -315.78  100 game average -225.50\n",
      "episode  4  score -43.39  100 game average -189.08\n",
      "episode  5  score -446.95  100 game average -232.06\n",
      "episode  6  score -96.03  100 game average -212.62\n",
      "episode  7  score -39.22  100 game average -190.95\n",
      "episode  8  score -342.53  100 game average -207.79\n",
      "episode  9  score -98.66  100 game average -196.88\n",
      "episode  10  score -62.83  100 game average -184.69\n",
      "episode  11  score -401.66  100 game average -202.77\n",
      "episode  12  score -136.60  100 game average -197.68\n",
      "episode  13  score -343.37  100 game average -208.09\n",
      "episode  14  score -229.67  100 game average -209.53\n",
      "episode  15  score -73.35  100 game average -201.02\n",
      "episode  16  score -350.54  100 game average -209.81\n",
      "episode  17  score -360.68  100 game average -218.19\n",
      "episode  18  score -280.22  100 game average -221.46\n",
      "episode  19  score -120.76  100 game average -216.42\n",
      "episode  20  score -113.78  100 game average -211.53\n",
      "episode  21  score -423.33  100 game average -221.16\n",
      "episode  22  score -94.02  100 game average -215.63\n",
      "episode  23  score -330.90  100 game average -220.44\n",
      "episode  24  score -449.21  100 game average -229.59\n",
      "episode  25  score -205.07  100 game average -228.64\n",
      "episode  26  score -250.22  100 game average -229.44\n",
      "episode  27  score -237.99  100 game average -229.75\n",
      "episode  28  score -63.39  100 game average -224.01\n",
      "episode  29  score -185.04  100 game average -222.71\n",
      "episode  30  score -305.63  100 game average -225.39\n",
      "episode  31  score -546.22  100 game average -235.41\n",
      "episode  32  score -130.54  100 game average -232.24\n",
      "episode  33  score -408.12  100 game average -237.41\n",
      "episode  34  score -289.95  100 game average -238.91\n",
      "episode  35  score -54.05  100 game average -233.78\n",
      "episode  36  score -316.27  100 game average -236.00\n",
      "episode  37  score -270.85  100 game average -236.92\n",
      "episode  38  score -437.81  100 game average -242.07\n",
      "episode  39  score -242.53  100 game average -242.08\n",
      "episode  40  score -58.51  100 game average -237.61\n",
      "episode  41  score -138.57  100 game average -235.25\n",
      "episode  42  score -256.62  100 game average -235.75\n",
      "episode  43  score -6.83  100 game average -230.54\n",
      "episode  44  score -73.43  100 game average -227.05\n",
      "episode  45  score -61.58  100 game average -223.45\n",
      "episode  46  score -341.64  100 game average -225.97\n",
      "episode  47  score -54.03  100 game average -222.39\n",
      "episode  48  score -102.62  100 game average -219.94\n",
      "episode  49  score -176.83  100 game average -219.08\n",
      "episode  50  score -50.63  100 game average -215.78\n",
      "episode  51  score -380.08  100 game average -218.94\n",
      "episode  52  score -414.09  100 game average -222.62\n",
      "episode  53  score -50.34  100 game average -219.43\n",
      "episode  54  score -52.35  100 game average -216.39\n",
      "episode  55  score -71.36  100 game average -213.80\n",
      "episode  56  score -256.86  100 game average -214.56\n",
      "episode  57  score -217.69  100 game average -214.61\n",
      "episode  58  score -350.16  100 game average -216.91\n",
      "episode  59  score -206.96  100 game average -216.74\n",
      "episode  60  score -394.05  100 game average -219.65\n",
      "episode  61  score -198.35  100 game average -219.31\n",
      "episode  62  score -147.86  100 game average -218.17\n",
      "episode  63  score -360.34  100 game average -220.39\n",
      "episode  64  score -337.08  100 game average -222.19\n",
      "episode  65  score -51.59  100 game average -219.60\n",
      "episode  66  score -29.18  100 game average -216.76\n",
      "episode  67  score -425.61  100 game average -219.83\n",
      "episode  68  score -264.15  100 game average -220.47\n",
      "episode  69  score -108.41  100 game average -218.87\n",
      "episode  70  score -92.74  100 game average -217.10\n",
      "episode  71  score -63.91  100 game average -214.97\n",
      "episode  72  score -377.07  100 game average -217.19\n",
      "episode  73  score -306.14  100 game average -218.39\n",
      "episode  74  score -327.56  100 game average -219.85\n",
      "episode  75  score -204.57  100 game average -219.65\n",
      "episode  76  score -22.95  100 game average -217.09\n",
      "episode  77  score -211.58  100 game average -217.02\n",
      "episode  78  score -87.46  100 game average -215.38\n",
      "episode  79  score -58.01  100 game average -213.41\n",
      "episode  80  score -269.66  100 game average -214.11\n",
      "episode  81  score -217.00  100 game average -214.14\n",
      "episode  82  score -187.70  100 game average -213.83\n",
      "episode  83  score -89.53  100 game average -212.35\n",
      "episode  84  score -47.23  100 game average -210.40\n",
      "episode  85  score -286.50  100 game average -211.29\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 244>\u001B[1;34m()\u001B[0m\n\u001B[0;32m    250\u001B[0m     action \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39maction_space\u001B[38;5;241m.\u001B[39msample()\n\u001B[0;32m    251\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 252\u001B[0m     action \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoose_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    253\u001B[0m index\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    254\u001B[0m new_state, reward, done, info \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mAgent.choose_action\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mchoose_action\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[1;32m--> 144\u001B[0m     actions,_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mactor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mnumpy()[\u001B[38;5;241m0\u001B[39m]\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mActionEstimator.evaluate\u001B[1;34m(self, state, reparameterize)\u001B[0m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, reparameterize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m---> 69\u001B[0m     mu, log_std \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     70\u001B[0m     sigma \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mexp(log_std)\n\u001B[0;32m     71\u001B[0m     action_dist \u001B[38;5;241m=\u001B[39m Normal(mu, sigma)\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mActionEstimator.forward\u001B[1;34m(self, state)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[1;32m---> 53\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSeqLayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m     mu \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmu(out)\n\u001B[0;32m     55\u001B[0m     log_std \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msigma(out)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 141\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "class SACBuffer:\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.counter = 0\n",
    "        self.states_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.actions_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.new_states_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.rewards_memory = np.zeros(self.mem_size)\n",
    "        self.dones = np.zeros(self.mem_size, dtype = np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        self.index = self.counter % self.mem_size\n",
    "        self.states_memory[index] = state\n",
    "        self.actions_memory[index] = action\n",
    "        self.rewards_memory[index] = reward\n",
    "        self.new_states_memory[index] = new_state\n",
    "        self.dones[index] = done\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = self.counter\n",
    "        if self.counter > self.mem_size:\n",
    "            max_mem = self.mem_size\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.states_memory[batch]\n",
    "        rewards = self.rewards_memory[batch]\n",
    "        new_states = self.new_states_memory[batch]\n",
    "        actions = self.actions_memory[batch]\n",
    "        dones = self.dones[batch]\n",
    "\n",
    "        return states, rewards, actions, new_states, dones\n",
    "\n",
    "class ActionEstimator(nn.Module):\n",
    "    def __init__(self, input_dims, actions, max_action, hidden_sizes):\n",
    "        super(ActionEstimator, self).__init__()\n",
    "        self.max_action = max_action\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.reparam_noise = 1e-6\n",
    "        self.LOG_STD_MAX = 2\n",
    "        self.LOG_STD_MIN = -20\n",
    "        self.SeqLayer = nn.Sequential(\n",
    "            nn.Linear(*input_dims, hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mu = nn.Linear(hidden_sizes[1], actions)\n",
    "        self.sigma = nn.Linear(hidden_sizes[1], actions)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def forward(self, state):\n",
    "        out = self.SeqLayer(state)\n",
    "        mu = self.mu(out)\n",
    "        log_std = self.sigma(out)\n",
    "        log_std = torch.clamp(log_std, self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        return mu, log_std\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        mu, log_std = self.forward(state)\n",
    "        sigma = torch.exp(log_std)\n",
    "        gaussian_dist = Normal(mu, sigma)\n",
    "        actions = gaussian_dist.rsample()\n",
    "        actions = torch.tanh(actions)\n",
    "        return actions.cpu().detach().numpy()[0]*self.max_action\n",
    "\n",
    "    def evaluate(self, state, reparameterize=False):\n",
    "        mu, log_std = self.forward(state)\n",
    "        sigma = torch.exp(log_std)\n",
    "        action_dist = Normal(mu, sigma)\n",
    "        if reparameterize:\n",
    "            action = action_dist.rsample()\n",
    "        else:\n",
    "            action = action_dist.sample()\n",
    "\n",
    "        # log_prob = action_dist.log_prob(action)-torch.log(1-action.pow(2)+self.reparam_noise)\n",
    "        # log_prob = log_prob.sum(dim=-1)\n",
    "        log_prob = action_dist.log_prob(action).sum(axis=-1)\n",
    "        log_prob -= (2*(np.log(2) - action - F.softplus(-2*action))).sum(axis=1)\n",
    "\n",
    "        return torch.tanh(action)*self.max_action, log_prob\n",
    "\n",
    "class Q_Net(nn.Module):\n",
    "    def __init__(self, input_dims, actions, hidden_sizes, learning_rate = 1e-3):\n",
    "        super(Q_Net, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.SeqLayer = nn.Sequential(\n",
    "            nn.Linear(input_dims[0]+actions, self.hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[0], self.hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[1], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, actions):\n",
    "        out = self.SeqLayer(torch.cat([state, actions], dim = -1))\n",
    "        return out\n",
    "\n",
    "class Value_Net(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_sizes, learning_rate = 1e-3):\n",
    "        super(Value_Net, self).__init__()\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.SeqLayer = nn.Sequential(\n",
    "            nn.Linear(input_dims[0], self.hidden_sizes[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[0], self.hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_sizes[1], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        out = self.SeqLayer(state)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self,  max_action, n_actions, input_dims, hidden_sizes = [256, 256], gamma=0.95, batch_size=256, learning_rate = 1e-3):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.gamma = gamma\n",
    "        self.max_action = max_action\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = 0.2\n",
    "        self.tau = 0.005\n",
    "        self.actor = ActionEstimator(input_dims, n_actions, max_action, hidden_sizes).to(self.device)\n",
    "        self.Q1 = Q_Net(input_dims, n_actions, hidden_sizes).to(self.device)\n",
    "        self.Q2 = Q_Net(input_dims, n_actions, hidden_sizes).to(self.device)\n",
    "        self.value_net = Value_Net(input_dims, hidden_sizes).to(self.device)\n",
    "        self.target_value_net = copy.deepcopy(self.value_net)\n",
    "\n",
    "        self.buffer = SACBuffer(1000000, input_shape = input_dims, n_actions = n_actions)\n",
    "        self.MSE_Criterion = nn.MSELoss()\n",
    "\n",
    "        self.V_params = itertools.chain(self.value_net.parameters())\n",
    "        self.V_targ_params = itertools.chain(self.target_value_net.parameters())\n",
    "        self.q1_optim = torch.optim.Adam(self.Q1.parameters(), lr=0.00003)\n",
    "        self.q2_optim = torch.optim.Adam(self.Q2.parameters(), lr=0.00003)\n",
    "        self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=0.00003)\n",
    "        self.v_optim = torch.optim.Adam(self.value_net.parameters(), lr=0.00003)\n",
    "        for p in self.V_targ_params:\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        actions,_ = self.actor.evaluate(state)\n",
    "        return actions.cpu().detach().numpy()[0]\n",
    "\n",
    "    def update_network_parameters(self):\n",
    "        target_value_params = self.target_value_net.named_parameters()\n",
    "        value_params = self.value_net.named_parameters()\n",
    "\n",
    "        target_value_state_dict = dict(target_value_params)\n",
    "        value_state_dict = dict(value_params)\n",
    "\n",
    "        for name in value_state_dict:\n",
    "            value_state_dict[name] = self.tau*value_state_dict[name].clone() + \\\n",
    "                                     (1-self.tau)*target_value_state_dict[name].clone()\n",
    "\n",
    "        self.target_value_net.load_state_dict(value_state_dict)\n",
    "\n",
    "    def remember(self, s, a, r, ns, done):\n",
    "        self.buffer.store_transition(s,a,r,ns,done)\n",
    "\n",
    "    def update(self):\n",
    "        if self.buffer.counter < self.batch_size:\n",
    "            return\n",
    "        states, rewards, actions, new_states, dones = self.buffer.sample_buffer(self.batch_size)\n",
    "\n",
    "        states_t = torch.from_numpy(states).float().to(self.device)\n",
    "        rewards_t = torch.from_numpy(rewards).float().to(self.device)\n",
    "        actions_t = torch.from_numpy(actions).float().to(self.device)\n",
    "        new_states_t = torch.from_numpy(new_states).float().to(self.device)\n",
    "        dones_t = torch.from_numpy(dones).to(self.device)\n",
    "\n",
    "\n",
    "        actions_pi, log_probs = self.actor.evaluate(states_t)\n",
    "        q1_value = self.Q1(states_t, actions_pi)\n",
    "        q2_value = self.Q2(states_t, actions_pi)\n",
    "        next_val = self.target_value_net(new_states_t).view(-1)\n",
    "        state_val = self.value_net(states_t).view(-1)\n",
    "        next_val[dones_t] = 0.0\n",
    "        predicted_new_q = torch.min(q1_value, q2_value).view(-1)\n",
    "\n",
    "        self.v_optim.zero_grad()\n",
    "        target_value_func = predicted_new_q - log_probs.view(-1)\n",
    "        value_loss = 0.5*self.MSE_Criterion(state_val, target_value_func.detach())\n",
    "        value_loss.backward(retain_graph=True)\n",
    "        self.v_optim.step()\n",
    "\n",
    "        # LOSS FOR ACTOR\n",
    "        actions_pip, log_probsp = self.actor.evaluate(states_t, reparameterize = True)\n",
    "        log_probsp = log_probsp.view(-1)\n",
    "\n",
    "        q1_new_policy = self.Q1(states_t, actions_pip)\n",
    "        q2_new_policy = self.Q2(states_t, actions_pip)\n",
    "        critic_value = torch.min(q1_new_policy, q2_new_policy)\n",
    "        critic_value = critic_value.view(-1)\n",
    "\n",
    "        policy_loss = (log_probsp - critic_value).mean()\n",
    "        self.actor_optim.zero_grad()\n",
    "        policy_loss.backward(retain_graph = True)\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # Freeze Q-networks so you don't waste computational effort\n",
    "        # computing gradients for them during the policy learning step.\n",
    "        self.q1_optim.zero_grad()\n",
    "        self.q2_optim.zero_grad()\n",
    "        q_hat = rewards_t + self.gamma * next_val\n",
    "        q1_old_policy = self.Q1(states_t, actions_t).view(-1)\n",
    "        q2_old_policy = self.Q2(states_t, actions_t).view(-1)\n",
    "        critic1_loss = 0.5*self.MSE_Criterion(q1_old_policy, q_hat)\n",
    "        critic2_loss = 0.5*self.MSE_Criterion(q2_old_policy, q_hat)\n",
    "\n",
    "        critic_loss = critic1_loss + critic2_loss\n",
    "        critic_loss.backward()\n",
    "        self.q1_optim.step()\n",
    "        self.q2_optim.step()\n",
    "\n",
    "        # Updating Policy\n",
    "\n",
    "        # Finally, update target networks by polyak averaging.\n",
    "        # with torch.no_grad():\n",
    "        #     for p_targ, p in zip(self.V_targ_params, self.V_params):\n",
    "        #         # NB: We use an in-place operations \"mul_\", \"add_\" to update target\n",
    "        #         # params, as opposed to \"mul\" and \"add\", which would make new tensors.\n",
    "        #         p_targ.data.mul_(self.polyak)\n",
    "        #         p_targ.data.add_((1 - self.polyak) * p.data)\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "writer = SummaryWriter()\n",
    "max_action = env.action_space.high[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "obs_dim = env.observation_space.shape\n",
    "n_games = 4000\n",
    "agent = Agent(max_action, n_actions, obs_dim)\n",
    "exploration_end = 10000\n",
    "start_updates = 1000\n",
    "update_steps = 50\n",
    "index = 0\n",
    "eps_rewards = []\n",
    "updating = 0\n",
    "for i in range(n_games):\n",
    "    done = False\n",
    "    score = 0\n",
    "    obs = env.reset()\n",
    "    while not done:\n",
    "        if index < exploration_end:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = agent.choose_action(obs)\n",
    "        index+=1\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        agent.remember(obs, action, reward, new_state, done)\n",
    "        if index > start_updates:\n",
    "            for j in range(5):\n",
    "                agent.update()\n",
    "        score += reward\n",
    "        obs = new_state\n",
    "    eps_rewards.append(score)\n",
    "    avg_score = np.mean(eps_rewards[-100:])\n",
    "    writer.add_scalar(\"Episode total reward\", eps_rewards[i], i)\n",
    "\n",
    "    print('episode ', i, \" score %.2f \" %score,\n",
    "          \"100 game average %.2f\" % avg_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}