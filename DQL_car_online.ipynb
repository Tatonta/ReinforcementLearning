{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "\n",
    "    def __init__(self, seed, layer_sizes, lr, sync_freq, exp_replay_size):\n",
    "        torch.manual_seed(seed)\n",
    "        self.q_net = self.build_nn(layer_sizes)\n",
    "        self.target_net = copy.deepcopy(self.q_net)\n",
    "        self.q_net.cuda()\n",
    "        self.target_net.cuda()\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.q_net.parameters(), lr=lr)\n",
    "\n",
    "        self.network_sync_freq = sync_freq\n",
    "        self.network_sync_counter = 0\n",
    "        self.gamma = torch.tensor(0.95).float().cuda()\n",
    "        self.experience_replay = deque(maxlen = exp_replay_size)\n",
    "        return\n",
    "\n",
    "    def build_nn(self, layer_sizes):\n",
    "        assert len(layer_sizes) > 1\n",
    "        layers = []\n",
    "        for index in range(len(layer_sizes)-1):\n",
    "            linear = nn.Linear(layer_sizes[index], layer_sizes[index+1])\n",
    "            act =    nn.Tanh() if index < len(layer_sizes)-2 else nn.Identity()\n",
    "            layers += (linear,act)\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def get_action(self, state, action_space_len, epsilon):\n",
    "        # We do not require gradient at this point, because this function will be used either\n",
    "        # during experience collection or during inference\n",
    "        with torch.no_grad():\n",
    "            Qp = self.q_net(torch.from_numpy(state).float().cuda())\n",
    "        Q,A = torch.max(Qp, axis=0)\n",
    "        A = A if torch.rand(1).item() > epsilon else torch.randint(0,action_space_len,(1,))\n",
    "        return A\n",
    "\n",
    "    def get_q_next(self, state):\n",
    "        with torch.no_grad():\n",
    "            qp = self.target_net(state)\n",
    "        q,_ = torch.max(qp, axis=1)\n",
    "        return q\n",
    "\n",
    "    def collect_experience(self, experience):\n",
    "        self.experience_replay.append(experience)\n",
    "        return\n",
    "\n",
    "    def sample_from_experience(self, sample_size):\n",
    "        if(len(self.experience_replay) < sample_size):\n",
    "            sample_size = len(self.experience_replay)\n",
    "        sample = random.sample(self.experience_replay, sample_size)\n",
    "        s = torch.tensor([exp[0] for exp in sample]).float()\n",
    "        a = torch.tensor([exp[1] for exp in sample]).float()\n",
    "        rn = torch.tensor([exp[2] for exp in sample]).float()\n",
    "        sn = torch.tensor([exp[3] for exp in sample]).float()\n",
    "        return s, a, rn, sn\n",
    "\n",
    "    def train(self, batch_size ):\n",
    "        s, a, rn, sn = self.sample_from_experience( sample_size = batch_size)\n",
    "        if(self.network_sync_counter == self.network_sync_freq):\n",
    "            self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "            self.network_sync_counter = 0\n",
    "\n",
    "        # predict expected return of current state using main network\n",
    "        qp = self.q_net(s.cuda())\n",
    "        pred_return, _ = torch.max(qp, axis=1)\n",
    "\n",
    "        # get target return using target network\n",
    "        q_next = self.get_q_next(sn.cuda())\n",
    "        target_return = rn.cuda() + self.gamma * q_next\n",
    "\n",
    "        loss = self.loss_fn(pred_return, target_return)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.network_sync_counter += 1\n",
    "        return loss.item()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]C:\\Users\\Tatonta\\AppData\\Local\\Temp\\ipykernel_4444\\1893061271.py:50: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:210.)\n",
      "  s = torch.tensor([exp[0] for exp in sample]).float()\n",
      " 71%|███████   | 7052/10000 [02:32<01:03, 46.19it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 27>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m(done \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m     30\u001B[0m     ep_len \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 31\u001B[0m     A \u001B[38;5;241m=\u001B[39m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m     obs_next, reward, done, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(A\u001B[38;5;241m.\u001B[39mitem())\n\u001B[0;32m     33\u001B[0m     agent\u001B[38;5;241m.\u001B[39mcollect_experience([obs, A\u001B[38;5;241m.\u001B[39mitem(), reward, obs_next])\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mDQN_Agent.get_action\u001B[1;34m(self, state, action_space_len, epsilon)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_action\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action_space_len, epsilon):\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;66;03m# We do not require gradient at this point, because this function will be used either\u001B[39;00m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# during experience collection or during inference\u001B[39;00m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 31\u001B[0m         Qp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mq_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m     Q,A \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(Qp, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     33\u001B[0m     A \u001B[38;5;241m=\u001B[39m A \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mrand(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m>\u001B[39m epsilon \u001B[38;5;28;01melse\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m,action_space_len,(\u001B[38;5;241m1\u001B[39m,))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    140\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 141\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1106\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1107\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1109\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1110\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1111\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1112\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "exp_replay_size = 256\n",
    "agent = DQN_Agent(seed = 1423, layer_sizes = [input_dim, 64, output_dim], lr = 1e-3, sync_freq = 5, exp_replay_size = exp_replay_size)\n",
    "\n",
    "# initiliaze experiance replay\n",
    "index = 0\n",
    "for i in range(exp_replay_size):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while(done != True):\n",
    "        A = agent.get_action(obs, env.action_space.n, epsilon=1)\n",
    "        obs_next, reward, done, _ = env.step(A.item())\n",
    "        agent.collect_experience([obs, A.item(), reward, obs_next])\n",
    "        obs = obs_next\n",
    "        index += 1\n",
    "        if( index > exp_replay_size ):\n",
    "            break\n",
    "\n",
    "# Main training loop\n",
    "losses_list, reward_list, episode_len_list, epsilon_list  = [], [], [], []\n",
    "index = 128\n",
    "episodes = 10000\n",
    "epsilon = 1\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs, done, losses, ep_len, rew = env.reset(), False, 0, 0, 0\n",
    "    while(done != True):\n",
    "        ep_len += 1\n",
    "        A = agent.get_action(obs, env.action_space.n, epsilon)\n",
    "        obs_next, reward, done, _ = env.step(A.item())\n",
    "        agent.collect_experience([obs, A.item(), reward, obs_next])\n",
    "\n",
    "        obs = obs_next\n",
    "        rew  += reward\n",
    "        index += 1\n",
    "\n",
    "        if(index > 128):\n",
    "            index = 0\n",
    "            for j in range(4):\n",
    "                loss = agent.train(batch_size=16)\n",
    "                losses += loss\n",
    "    if epsilon > 0.05 :\n",
    "        epsilon -= (1 / 5000)\n",
    "\n",
    "    losses_list.append(losses/ep_len), reward_list.append(rew), episode_len_list.append(ep_len), epsilon_list.append(epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241m.\u001B[39mplot(reward_list)\n\u001B[0;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mshow()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.plot(reward_list)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "while True:\n",
    "    action = agent.get_action(obs, env.action_space.n, 0)\n",
    "\n",
    "    obs, _, done, _=env.step(action.item())\n",
    "    env.render()\n",
    "    if done:\n",
    "        env.reset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}